# Nontriviality

This document explains why SuayLang’s core research claim (interpreter ↔ VM observational equivalence, measured by a deterministic pipeline) is not a trivial engineering exercise.

## Nontrivial obstacle 1: defining “equivalence” without cheating

Two runtimes can be made to “match” by picking an observation that ignores the differences.
SuayLang addresses this by making the observation policy explicit and reviewable:

- **Observation policy:** what outputs, errors, and spans are compared; what is intentionally ignored.
- **Falsification hook:** the policy defines what constitutes a counterexample.

Concrete artifact:
- Policy text: docs/OBSERVATION_POLICY.md
- Falsifier output: results/diff_report.md (generated by `make reproduce-all`)

## Nontrivial obstacle 2: error locations must align across implementations

A common failure mode in multi-backend languages is that both backends fail, but at different spans (or with different codes), making “equivalence” ambiguous.
SuayLang treats diagnostics as part of semantics:

- Stable error codes (contract): docs/ERROR_CODES.md
- Span policy (where caret points): docs/DIAGNOSTICS_CONTRACT.md
- Regression tests assert spans/codes for representative cases.

What breaks if done naively:
- A backend can be “equivalent” only by producing generic errors (loss of semantic information).
- Reviewers cannot trust counterexample localization.

## Nontrivial obstacle 3: stack VM compilation must preserve evaluation order

A stack VM compilation strategy can silently change evaluation order:

- short-circuit operators (`∧`, `∨`)
- dispatch/cycle control constructs
- blocks and environment creation

If evaluation order drifts, you can get spurious equivalence passes on pure programs and failures on programs with errors.
SuayLang makes evaluation order explicit in the specification and treats it as testable.

## Nontrivial obstacle 4: determinism is a property, not an accident

Research artifacts fail committee review when “reproduce” is non-deterministic.
SuayLang treats determinism as a declared guarantee:

- Deterministic seeds (when generation/fuzzing is used)
- Artifact hashing (`results/manifest.json`, `results/hashes.txt`)
- A single canonical pipeline (`make reproduce-all`) with explicit inputs/outputs

What breaks if done naively:
- Plots/tables drift between runs, masking real regressions.
- Independent reproduction becomes a social process instead of a mechanical check.

## Nontrivial obstacle 5: avoiding shared-oracle false negatives

Interpreter and VM can share logic indirectly (same parser, same normalization, same observation policy implementation).
This creates a risk of shared-oracle bugs.

Mitigations in this repo:
- Make the observation policy reviewable as a document.
- Keep evidence artifacts file-based and auditable.
- Track negative results and threats explicitly.

See also:
- docs/THREATS_TO_VALIDITY.md
- docs/NEGATIVE_RESULTS.md

# Micro-evaluation of control flow design (SuayLang v0.1)

This document defines a lightweight, rubric-based micro-evaluation intended for skeptical academic review.
It is not a user study; it is a transparent, reproducible comparison using small tasks and explicit metrics.

## Baseline language

Primary baseline: **Python** (statement-oriented control flow; widely understood by reviewers).

Notes:
- Rust and Scheme are plausible secondary baselines, but are not included in the initial measured table to avoid “pseudo-baseline” comparisons without comparable constraints.
- Adding Rust/Scheme later is treated as future work.

## Tasks (fixed)

Each task is implemented in:
- SuayLang (v0.1)
- Python (baseline)

Tasks:
1) **Branching-heavy logic:** classify a value into categories (nested decisions).
2) **Iterative state evolution:** implement a small state machine (explicit termination).
3) **Interpreter-like logic:** evaluate a tiny tagged AST (variants + dispatch).

Source files:
- SuayLang: docs/research/tasks/*.suay
- Python: docs/research/tasks/baselines/*.py

## Metrics

### M1 — Lines of code (LOC)
- Count non-empty, non-comment lines.
- Purpose: descriptive only.

### M2 — Control construct count
- SuayLang constructs: `▷`, `⟲`, `↩`, `↯`.
- Python constructs: `if`, `elif`, `else`, `while`, `for`, `match`, `case`, `break`, `continue`.

### M3 — Local reasoning steps (explicit rubric)
A *local reasoning step* is counted when a reader must account for an implicit control transition.

Rubric (conservative):
- +1 per control-flow decision point (branch arm selection).
- +1 per loop transition where the next state is implicit rather than explicit.
- +1 per non-local exit mechanism (break/continue/early return) that changes the control destination.

Interpretation:
- `dispatch` arms are explicit decision points; each arm is counted as 1 decision point.
- `cycle` transitions are explicit; each `↩`/`↯` is counted as 1 transition.
- Python loops often require reasoning about both the loop condition and the body’s mutation; those are counted explicitly in notes.

This rubric is intentionally simple and transparent; it does not pretend to measure comprehension.

## Procedure (reproducible)

Run:

```sh
python scripts/micro_eval.py
```

This produces a small table of M1/M2 for each task. M3 is reported as a manual rubric annotation in this document.

## Results (initial, measured counts)

The table below is generated by scripts/micro_eval.py.

(See output when running the script locally; results are expected to be stable unless the task sources change.)

## Limitations

- No human subjects: cannot claim comprehension improvements.
- Keyword counting is a proxy: does not capture nesting or semantic difficulty.
- Unicode confounds readability: see docs/research/UNICODE_VARIABLE.md.
